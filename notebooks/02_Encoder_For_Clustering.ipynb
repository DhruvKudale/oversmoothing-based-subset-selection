{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0d682f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "72c692c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttentionLayer(nn.Module):\n",
    "    def __init__(self, input_dim, num_heads=1):\n",
    "        super(SelfAttentionLayer, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = input_dim // num_heads\n",
    "        \n",
    "        self.query = nn.Linear(input_dim, input_dim)\n",
    "        self.key = nn.Linear(input_dim, input_dim)\n",
    "        self.value = nn.Linear(input_dim, input_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        \n",
    "        # Split input into heads\n",
    "        query = self.query(x).view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
    "        key = self.key(x).view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
    "        value = self.value(x).view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
    "        \n",
    "        # Compute attention scores\n",
    "        attention_scores = torch.matmul(query, key.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.head_dim, dtype=torch.float32))\n",
    "        attention_weights = torch.softmax(attention_scores, dim=-1)\n",
    "        \n",
    "        # Apply attention to values\n",
    "        attention_output = torch.matmul(attention_weights, value)\n",
    "        attention_output = attention_output.view(batch_size, seq_len, -1)\n",
    "        \n",
    "        return attention_output\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, num_layers=1, num_heads=1, hidden_dim=64):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.self_attention_layers = nn.ModuleList([\n",
    "            SelfAttentionLayer(input_dim, num_heads) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.feedforward_layers = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(input_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim, input_dim)\n",
    "            ) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.layer_norms = nn.ModuleList([nn.LayerNorm(input_dim) for _ in range(num_layers)])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for i in range(self.num_layers):\n",
    "            # Self-attention layer\n",
    "            attention_output = self.self_attention_layers[i](x)\n",
    "            # Add residual connection and apply layer normalization\n",
    "            x = self.layer_norms[i](x + attention_output)\n",
    "            # Feedforward layer\n",
    "            feedforward_output = self.feedforward_layers[i](x)\n",
    "            # Add residual connection and apply layer normalization\n",
    "            x = self.layer_norms[i](x + feedforward_output)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c76e9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init model with input_dim\n",
    "input_dim = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab49239c",
   "metadata": {},
   "source": [
    "### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "0c2feeb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "import tqdm\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "e4c9eb7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since glue couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'sst2' at /home/dhruv/.cache/huggingface/datasets/glue/sst2/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c (last modified on Thu Apr 11 16:26:56 2024).\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"glue\", \"sst2\")\n",
    "train_dataset = dataset[\"train\"]\n",
    "val_dataset = dataset[\"validation\"]\n",
    "test_dataset = dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "6737b222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read pkl file \n",
    "with open('sst-train-64.pkl', 'rb') as f:\n",
    "    vectors = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "570666b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(67349, 64)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "3a8bb805",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_distance_between_vectors(vectors):\n",
    "    \"\"\"\n",
    "    Calculate the average distance between all pairs of vectors within a set.\n",
    "\n",
    "    Parameters:\n",
    "    vectors (numpy.ndarray): A 2D numpy array where each row represents a vector.\n",
    "\n",
    "    Returns:\n",
    "    float: The average distance between all pairs of vectors.\n",
    "    \"\"\"\n",
    "    # Calculate pairwise distances using broadcasting\n",
    "    pairwise_distances = np.linalg.norm(vectors[:, None] - vectors, axis=-1)\n",
    "    \n",
    "    # Exclude diagonal elements (distances between the same vectors)\n",
    "    pairwise_distances = np.triu(pairwise_distances, k=1)\n",
    "    \n",
    "    # Calculate the average distance\n",
    "    average_distance = np.mean(pairwise_distances)\n",
    "    \n",
    "    return average_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d648e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 15637/67349 [04:39<16:57, 50.80it/s]"
     ]
    }
   ],
   "source": [
    "N = 128\n",
    "X = []\n",
    "encoder = Encoder(input_dim, num_layers = N, num_heads = 8, hidden_dim = 64)\n",
    "for i in tqdm.tqdm(range(len(train_dataset))):\n",
    "    # Sample usage\n",
    "    input_sequence = torch.tensor(vectors[i].reshape(1, 1, 64))\n",
    "    output_sequence = encoder(input_sequence)[0][0].detach().numpy()\n",
    "    X.append(output_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "e54b8cf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.348753\n"
     ]
    }
   ],
   "source": [
    "X_np = np.array(X)\n",
    "dist = average_distance_between_vectors(np.mean(X_np, axis = 0))\n",
    "print(dist)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
