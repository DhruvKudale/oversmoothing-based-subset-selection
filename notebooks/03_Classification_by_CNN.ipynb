{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "import tqdm\n",
    "import math\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # GLUE\n",
    "# dataset = load_dataset(\"glue\", \"sst2\")\n",
    "# val_dataset = dataset[\"validation\"]\n",
    "\n",
    "dataset = load_dataset(\"christophsonntag/OLID\")\n",
    "train_dataset = dataset[\"train\"]\n",
    "test_dataset = dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Based Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CNN architecture for 1D vectors\n",
    "class CNNClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNClassifier, self).__init__()\n",
    "#         self.conv1 = nn.Conv1d(in_channels = 1, out_channels = 32, kernel_size = 3)\n",
    "#         self.conv2 = nn.Conv1d(in_channels = 32, out_channels = 32, kernel_size = 3)\n",
    "#         self.conv3 = nn.Conv1d(in_channels = 32, out_channels = 16, kernel_size = 3)\n",
    "#         self.pool = nn.MaxPool1d(kernel_size = 2)\n",
    "        self.fc1 = nn.Linear(768, 4)  # Adjust input size according to your data\n",
    "        self.fc2 = nn.Linear(4, 1)      # Output layer with 1 neuron for binary classification\n",
    "\n",
    "    def forward(self, x):\n",
    "#         x = self.pool(nn.functional.relu(self.conv1(x)))\n",
    "#         x = self.pool(nn.functional.relu(self.conv2(x)))\n",
    "#         x = self.pool(nn.functional.relu(self.conv3(x)))\n",
    "#         x = x.view(-1, 16 * 94)  # Adjust the size according to the output size of the last convolutional layer\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = torch.sigmoid(self.fc2(x))  # Sigmoid activation for binary classification\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data loader for demonstration\n",
    "class SampleDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Training Data\n",
    "\n",
    "# Read pkl file \n",
    "with open('embeddings/OLID-MPNET/OLID-train-768.pkl', 'rb') as f:\n",
    "    vectors = pickle.load(f)\n",
    "    \n",
    "X = np.array(vectors)\n",
    "\n",
    "labels = []\n",
    "for item in train_dataset:\n",
    "    if item['subtask_a'] == 'OFF':\n",
    "        label = 0\n",
    "    else:\n",
    "        label = 1\n",
    "    labels.append(label)\n",
    "    \n",
    "Y = np.array(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determining which Subset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Instance Scores\n",
    "with open('scores/OLID-MPNET/instance-scores-OLID-L1-30-40-30.pkl', 'rb') as f:\n",
    "    instance_scores = pickle.load(f)\n",
    "    \n",
    "# Sort indices based on the score in descending order\n",
    "def sort_indices_by_values(values):\n",
    "    return sorted(range(len(values)), key=lambda i: np.abs(values[i]), reverse = True)\n",
    "\n",
    "# Example usage:\n",
    "sorted_indices = sort_indices_by_values(instance_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute subset length\n",
    "\n",
    "percentage = 0.05\n",
    "name = 'models/OLID/model-L1-5.pth' \n",
    "\n",
    "subset_len = int(percentage * len(X))\n",
    "sorted_subset_indices = sorted_indices[:subset_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_eff = []\n",
    "Y_eff = []\n",
    "\n",
    "for i in sorted_subset_indices:\n",
    "    X_eff.append(X[i])\n",
    "    Y_eff.append(Y[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "662 samples selected\n"
     ]
    }
   ],
   "source": [
    "N = len(X_eff)\n",
    "X_train = torch.from_numpy(np.array(X_eff).reshape(N, 1, 768))\n",
    "Y_train = torch.from_numpy(np.array(Y_eff).reshape(N, 1, 1)).float()\n",
    "print(str(N) + ' samples selected')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0: 163\n",
      "Class 1: 499\n"
     ]
    }
   ],
   "source": [
    "c0 = 0\n",
    "c1 = 0\n",
    "for i in Y_eff:\n",
    "    if i == 0:\n",
    "        c0 += 1\n",
    "    else:\n",
    "        c1 += 1\n",
    "        \n",
    "print('Class 0: ' + str(c0))\n",
    "print('Class 1: ' + str(c1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 21/200 [00:00<00:01, 104.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    10] loss: 0.0607\n",
      "[2,    10] loss: 0.0602\n",
      "[3,    10] loss: 0.0594\n",
      "[4,    10] loss: 0.0603\n",
      "[5,    10] loss: 0.0600\n",
      "[6,    10] loss: 0.0585\n",
      "[7,    10] loss: 0.0582\n",
      "[8,    10] loss: 0.0601\n",
      "[9,    10] loss: 0.0599\n",
      "[10,    10] loss: 0.0584\n",
      "[11,    10] loss: 0.0582\n",
      "[12,    10] loss: 0.0594\n",
      "[13,    10] loss: 0.0577\n",
      "[14,    10] loss: 0.0566\n",
      "[15,    10] loss: 0.0570\n",
      "[16,    10] loss: 0.0570\n",
      "[17,    10] loss: 0.0590\n",
      "[18,    10] loss: 0.0557\n",
      "[19,    10] loss: 0.0571\n",
      "[20,    10] loss: 0.0562\n",
      "[21,    10] loss: 0.0565\n",
      "[22,    10] loss: 0.0556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 16%|█▌        | 32/200 [00:00<00:01, 98.67it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23,    10] loss: 0.0544\n",
      "[24,    10] loss: 0.0555\n",
      "[25,    10] loss: 0.0548\n",
      "[26,    10] loss: 0.0539\n",
      "[27,    10] loss: 0.0564\n",
      "[28,    10] loss: 0.0538\n",
      "[29,    10] loss: 0.0545\n",
      "[30,    10] loss: 0.0526\n",
      "[31,    10] loss: 0.0546\n",
      "[32,    10] loss: 0.0536\n",
      "[33,    10] loss: 0.0524\n",
      "[34,    10] loss: 0.0539\n",
      "[35,    10] loss: 0.0513\n",
      "[36,    10] loss: 0.0519\n",
      "[37,    10] loss: 0.0525\n",
      "[38,    10] loss: 0.0521\n",
      "[39,    10] loss: 0.0526\n",
      "[40,    10] loss: 0.0510\n",
      "[41,    10] loss: 0.0510\n",
      "[42,    10] loss: 0.0517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 54/200 [00:00<00:01, 101.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[43,    10] loss: 0.0496\n",
      "[44,    10] loss: 0.0503\n",
      "[45,    10] loss: 0.0497\n",
      "[46,    10] loss: 0.0505\n",
      "[47,    10] loss: 0.0495\n",
      "[48,    10] loss: 0.0491\n",
      "[49,    10] loss: 0.0496\n",
      "[50,    10] loss: 0.0497\n",
      "[51,    10] loss: 0.0483\n",
      "[52,    10] loss: 0.0473\n",
      "[53,    10] loss: 0.0477\n",
      "[54,    10] loss: 0.0475\n",
      "[55,    10] loss: 0.0462\n",
      "[56,    10] loss: 0.0469\n",
      "[57,    10] loss: 0.0457\n",
      "[58,    10] loss: 0.0483\n",
      "[59,    10] loss: 0.0464\n",
      "[60,    10] loss: 0.0460\n",
      "[61,    10] loss: 0.0471\n",
      "[62,    10] loss: 0.0442\n",
      "[63,    10] loss: 0.0452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 76/200 [00:00<00:01, 100.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[64,    10] loss: 0.0459\n",
      "[65,    10] loss: 0.0463\n",
      "[66,    10] loss: 0.0442\n",
      "[67,    10] loss: 0.0458\n",
      "[68,    10] loss: 0.0448\n",
      "[69,    10] loss: 0.0457\n",
      "[70,    10] loss: 0.0441\n",
      "[71,    10] loss: 0.0446\n",
      "[72,    10] loss: 0.0431\n",
      "[73,    10] loss: 0.0440\n",
      "[74,    10] loss: 0.0411\n",
      "[75,    10] loss: 0.0453\n",
      "[76,    10] loss: 0.0454\n",
      "[77,    10] loss: 0.0439\n",
      "[78,    10] loss: 0.0420\n",
      "[79,    10] loss: 0.0430\n",
      "[80,    10] loss: 0.0436\n",
      "[81,    10] loss: 0.0425\n",
      "[82,    10] loss: 0.0433\n",
      "[83,    10] loss: 0.0413\n",
      "[84,    10] loss: 0.0411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 98/200 [00:00<00:01, 98.75it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[85,    10] loss: 0.0403\n",
      "[86,    10] loss: 0.0389\n",
      "[87,    10] loss: 0.0383\n",
      "[88,    10] loss: 0.0424\n",
      "[89,    10] loss: 0.0408\n",
      "[90,    10] loss: 0.0414\n",
      "[91,    10] loss: 0.0392\n",
      "[92,    10] loss: 0.0414\n",
      "[93,    10] loss: 0.0407\n",
      "[94,    10] loss: 0.0393\n",
      "[95,    10] loss: 0.0397\n",
      "[96,    10] loss: 0.0390\n",
      "[97,    10] loss: 0.0392\n",
      "[98,    10] loss: 0.0398\n",
      "[99,    10] loss: 0.0394\n",
      "[100,    10] loss: 0.0378\n",
      "[101,    10] loss: 0.0371\n",
      "[102,    10] loss: 0.0396\n",
      "[103,    10] loss: 0.0384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████▉    | 119/200 [00:01<00:00, 97.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[104,    10] loss: 0.0362\n",
      "[105,    10] loss: 0.0369\n",
      "[106,    10] loss: 0.0375\n",
      "[107,    10] loss: 0.0372\n",
      "[108,    10] loss: 0.0367\n",
      "[109,    10] loss: 0.0357\n",
      "[110,    10] loss: 0.0395\n",
      "[111,    10] loss: 0.0360\n",
      "[112,    10] loss: 0.0365\n",
      "[113,    10] loss: 0.0370\n",
      "[114,    10] loss: 0.0343\n",
      "[115,    10] loss: 0.0353\n",
      "[116,    10] loss: 0.0346\n",
      "[117,    10] loss: 0.0360\n",
      "[118,    10] loss: 0.0365\n",
      "[119,    10] loss: 0.0366\n",
      "[120,    10] loss: 0.0363\n",
      "[121,    10] loss: 0.0359\n",
      "[122,    10] loss: 0.0373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 140/200 [00:01<00:00, 95.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[123,    10] loss: 0.0365\n",
      "[124,    10] loss: 0.0341\n",
      "[125,    10] loss: 0.0362\n",
      "[126,    10] loss: 0.0341\n",
      "[127,    10] loss: 0.0349\n",
      "[128,    10] loss: 0.0351\n",
      "[129,    10] loss: 0.0347\n",
      "[130,    10] loss: 0.0337\n",
      "[131,    10] loss: 0.0365\n",
      "[132,    10] loss: 0.0353\n",
      "[133,    10] loss: 0.0372\n",
      "[134,    10] loss: 0.0370\n",
      "[135,    10] loss: 0.0353\n",
      "[136,    10] loss: 0.0330\n",
      "[137,    10] loss: 0.0322\n",
      "[138,    10] loss: 0.0326\n",
      "[139,    10] loss: 0.0333\n",
      "[140,    10] loss: 0.0344\n",
      "[141,    10] loss: 0.0330\n",
      "[142,    10] loss: 0.0338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 162/200 [00:01<00:00, 97.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[143,    10] loss: 0.0346\n",
      "[144,    10] loss: 0.0318\n",
      "[145,    10] loss: 0.0300\n",
      "[146,    10] loss: 0.0321\n",
      "[147,    10] loss: 0.0320\n",
      "[148,    10] loss: 0.0327\n",
      "[149,    10] loss: 0.0308\n",
      "[150,    10] loss: 0.0300\n",
      "[151,    10] loss: 0.0335\n",
      "[152,    10] loss: 0.0338\n",
      "[153,    10] loss: 0.0318\n",
      "[154,    10] loss: 0.0313\n",
      "[155,    10] loss: 0.0345\n",
      "[156,    10] loss: 0.0326\n",
      "[157,    10] loss: 0.0320\n",
      "[158,    10] loss: 0.0308\n",
      "[159,    10] loss: 0.0345\n",
      "[160,    10] loss: 0.0328\n",
      "[161,    10] loss: 0.0299\n",
      "[162,    10] loss: 0.0305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 86%|████████▋ | 173/200 [00:01<00:00, 98.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[163,    10] loss: 0.0313\n",
      "[164,    10] loss: 0.0293\n",
      "[165,    10] loss: 0.0279\n",
      "[166,    10] loss: 0.0294\n",
      "[167,    10] loss: 0.0325\n",
      "[168,    10] loss: 0.0287\n",
      "[169,    10] loss: 0.0321\n",
      "[170,    10] loss: 0.0298\n",
      "[171,    10] loss: 0.0317\n",
      "[172,    10] loss: 0.0290\n",
      "[173,    10] loss: 0.0314\n",
      "[174,    10] loss: 0.0277\n",
      "[175,    10] loss: 0.0300\n",
      "[176,    10] loss: 0.0297\n",
      "[177,    10] loss: 0.0289\n",
      "[178,    10] loss: 0.0303\n",
      "[179,    10] loss: 0.0304\n",
      "[180,    10] loss: 0.0306\n",
      "[181,    10] loss: 0.0320\n",
      "[182,    10] loss: 0.0287\n",
      "[183,    10] loss: 0.0322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:02<00:00, 98.46it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[184,    10] loss: 0.0296\n",
      "[185,    10] loss: 0.0296\n",
      "[186,    10] loss: 0.0263\n",
      "[187,    10] loss: 0.0284\n",
      "[188,    10] loss: 0.0258\n",
      "[189,    10] loss: 0.0267\n",
      "[190,    10] loss: 0.0281\n",
      "[191,    10] loss: 0.0291\n",
      "[192,    10] loss: 0.0302\n",
      "[193,    10] loss: 0.0270\n",
      "[194,    10] loss: 0.0273\n",
      "[195,    10] loss: 0.0238\n",
      "[196,    10] loss: 0.0277\n",
      "[197,    10] loss: 0.0241\n",
      "[198,    10] loss: 0.0276\n",
      "[199,    10] loss: 0.0288\n",
      "[200,    10] loss: 0.0269\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model, loss function, and optimizer\n",
    "model = CNNClassifier()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.00005)\n",
    "\n",
    "# Create data loader\n",
    "trainset = SampleDataset(X_train, Y_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size = 32, shuffle=True)\n",
    "\n",
    "# Train the model\n",
    "for epoch in tqdm.tqdm(range(200)):  # Adjust number of epochs as needed\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 9:  # Print every 10 mini-batches\n",
    "            print('[%d, %5d] loss: %.4f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 100))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNNClassifier(\n",
       "  (fc1): Linear(in_features=768, out_features=4, bias=True)\n",
       "  (fc2): Linear(in_features=4, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the trained model\n",
    "torch.save(model.state_dict(), name)\n",
    "\n",
    "# Perform inference with the saved model\n",
    "loaded_model = CNNClassifier()\n",
    "loaded_model.load_state_dict(torch.load(name))\n",
    "loaded_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets do testing\n",
    "\n",
    "# Example inference\n",
    "# sample_input = torch.randn(1, 1, 768)  # Example input\n",
    "# output = loaded_model(sample_input)\n",
    "# print(\"Model output:\", output.item())\n",
    "\n",
    "# Read pkl file \n",
    "with open('embeddings/OLID-MPNET/OLID-test-768.pkl', 'rb') as f:\n",
    "    test_vectors = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 860/860 [00:00<00:00, 10256.80it/s]\n"
     ]
    }
   ],
   "source": [
    "gt = []\n",
    "preds = []\n",
    "\n",
    "for i in tqdm.tqdm(range(len(test_vectors))):\n",
    "    input_vector = torch.tensor(test_vectors[i].reshape(1, 1, 768))\n",
    "    label = test_dataset[i]['subtask_a']\n",
    "    if label == 'OFF':\n",
    "        actual = 0\n",
    "    else:\n",
    "        actual = 1\n",
    "    output = loaded_model(input_vector) \n",
    "    if output < 0.5:\n",
    "        pred = 0\n",
    "    else:\n",
    "        pred = 1\n",
    "    gt.append(actual)\n",
    "    preds.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "205\n"
     ]
    }
   ],
   "source": [
    "# Print the number of misclassifications\n",
    "misc = 0\n",
    "for i in range(len(gt)):\n",
    "    if gt[i] != preds[i]:\n",
    "        misc += 1\n",
    "\n",
    "print(misc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.28      0.40       240\n",
      "           1       0.77      0.95      0.85       620\n",
      "\n",
      "    accuracy                           0.76       860\n",
      "   macro avg       0.72      0.62      0.63       860\n",
      "weighted avg       0.75      0.76      0.73       860\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Generate classification report\n",
    "report = classification_report(gt, preds)\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
