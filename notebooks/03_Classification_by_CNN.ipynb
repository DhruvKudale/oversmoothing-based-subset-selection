{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "import tqdm\n",
    "import math\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"glue\", \"sst2\")\n",
    "train_dataset = dataset[\"train\"]\n",
    "val_dataset = dataset[\"validation\"]\n",
    "test_dataset = dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Based Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CNN architecture for 1D vectors\n",
    "class CNNClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNClassifier, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels = 1, out_channels = 16, kernel_size=3)\n",
    "        self.conv2 = nn.Conv1d(in_channels = 16, out_channels = 16, kernel_size=3)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2)\n",
    "        self.fc1 = nn.Linear(16 * 190, 128)  # Adjust input size according to your data\n",
    "        self.fc2 = nn.Linear(128, 1)      # Output layer with 1 neuron for binary classification\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(nn.functional.relu(self.conv1(x)))\n",
    "        x = self.pool(nn.functional.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 190)  # Adjust the size according to the output size of the last convolutional layer\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = torch.sigmoid(self.fc2(x))  # Sigmoid activation for binary classification\n",
    "        return x.unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data loader for demonstration\n",
    "class SampleDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Training Data\n",
    "\n",
    "# Read pkl file \n",
    "with open('sst-train-768.pkl', 'rb') as f:\n",
    "    vectors = pickle.load(f)\n",
    "    \n",
    "X = np.array(vectors)\n",
    "\n",
    "labels = []\n",
    "for item in train_dataset:\n",
    "    labels.append(item['label'])\n",
    "    \n",
    "Y = np.array(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determining which Subset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Instance Scores\n",
    "with open('scores/instance-scores-L0.pkl', 'rb') as f:\n",
    "    instance_scores = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_eff = []\n",
    "Y_eff = []\n",
    "\n",
    "M = len(instance_scores)\n",
    "for i in range(M):\n",
    "    if instance_scores[i] > 0:\n",
    "        X_eff.append(X[i])\n",
    "        Y_eff.append(Y[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58896 samples selected\n"
     ]
    }
   ],
   "source": [
    "N = len(X_eff)\n",
    "X_train = torch.from_numpy(np.array(X_eff).reshape(N, 1, 768))\n",
    "Y_train = torch.from_numpy(np.array(Y_eff).reshape(N, 1, 1)).float()\n",
    "print(str(N) + ' samples selected')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0: 26390\n",
      "Class 1: 32506\n"
     ]
    }
   ],
   "source": [
    "c0 = 0\n",
    "c1 = 0\n",
    "for i in Y_eff:\n",
    "    if i == 0:\n",
    "        c0 += 1\n",
    "    else:\n",
    "        c1 += 1\n",
    "        \n",
    "print('Class 0: ' + str(c0))\n",
    "print('Class 1: ' + str(c1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    10] loss: 0.691\n",
      "[1,   110] loss: 6.873\n",
      "[1,   210] loss: 6.840\n",
      "[1,   310] loss: 6.857\n",
      "[1,   410] loss: 6.858\n",
      "[1,   510] loss: 6.856\n",
      "[1,   610] loss: 6.823\n",
      "[1,   710] loss: 6.853\n",
      "[1,   810] loss: 6.836\n",
      "[1,   910] loss: 6.783\n",
      "[1,  1010] loss: 6.769\n",
      "[1,  1110] loss: 6.709\n",
      "[1,  1210] loss: 6.633\n",
      "[1,  1310] loss: 6.459\n",
      "[1,  1410] loss: 6.296\n",
      "[1,  1510] loss: 5.966\n",
      "[1,  1610] loss: 5.480\n",
      "[1,  1710] loss: 4.874\n",
      "[1,  1810] loss: 4.193\n",
      "[1,  1910] loss: 3.661\n",
      "[1,  2010] loss: 3.227\n",
      "[1,  2110] loss: 2.842\n",
      "[1,  2210] loss: 2.409\n",
      "[1,  2310] loss: 2.263\n",
      "[1,  2410] loss: 1.962\n",
      "[1,  2510] loss: 1.965\n",
      "[1,  2610] loss: 1.816\n",
      "[1,  2710] loss: 1.682\n",
      "[1,  2810] loss: 1.727\n",
      "[1,  2910] loss: 1.582\n",
      "[1,  3010] loss: 1.538\n",
      "[1,  3110] loss: 1.397\n",
      "[1,  3210] loss: 1.415\n",
      "[1,  3310] loss: 1.353\n",
      "[1,  3410] loss: 1.285\n",
      "[1,  3510] loss: 1.285\n",
      "[1,  3610] loss: 1.274\n",
      "[2,    10] loss: 0.115\n",
      "[2,   110] loss: 1.225\n",
      "[2,   210] loss: 1.222\n",
      "[2,   310] loss: 1.196\n",
      "[2,   410] loss: 1.142\n",
      "[2,   510] loss: 0.976\n",
      "[2,   610] loss: 1.149\n",
      "[2,   710] loss: 1.167\n",
      "[2,   810] loss: 1.169\n",
      "[2,   910] loss: 1.094\n",
      "[2,  1010] loss: 1.023\n",
      "[2,  1110] loss: 1.053\n",
      "[2,  1210] loss: 0.855\n",
      "[2,  1310] loss: 1.031\n",
      "[2,  1410] loss: 0.796\n",
      "[2,  1510] loss: 0.965\n",
      "[2,  1610] loss: 1.019\n",
      "[2,  1710] loss: 1.002\n",
      "[2,  1810] loss: 0.994\n",
      "[2,  1910] loss: 0.994\n",
      "[2,  2010] loss: 1.022\n",
      "[2,  2110] loss: 0.886\n",
      "[2,  2210] loss: 1.031\n",
      "[2,  2310] loss: 0.800\n",
      "[2,  2410] loss: 0.919\n",
      "[2,  2510] loss: 0.940\n",
      "[2,  2610] loss: 1.011\n",
      "[2,  2710] loss: 0.978\n",
      "[2,  2810] loss: 0.869\n",
      "[2,  2910] loss: 0.887\n",
      "[2,  3010] loss: 0.849\n",
      "[2,  3110] loss: 0.913\n",
      "[2,  3210] loss: 0.904\n",
      "[2,  3310] loss: 0.774\n",
      "[2,  3410] loss: 0.898\n",
      "[2,  3510] loss: 0.847\n",
      "[2,  3610] loss: 0.896\n",
      "[3,    10] loss: 0.104\n",
      "[3,   110] loss: 0.847\n",
      "[3,   210] loss: 0.738\n",
      "[3,   310] loss: 0.781\n",
      "[3,   410] loss: 0.829\n",
      "[3,   510] loss: 0.881\n",
      "[3,   610] loss: 0.823\n",
      "[3,   710] loss: 0.824\n",
      "[3,   810] loss: 0.819\n",
      "[3,   910] loss: 0.804\n",
      "[3,  1010] loss: 0.948\n",
      "[3,  1110] loss: 0.939\n",
      "[3,  1210] loss: 0.747\n",
      "[3,  1310] loss: 0.730\n",
      "[3,  1410] loss: 0.765\n",
      "[3,  1510] loss: 0.852\n",
      "[3,  1610] loss: 0.857\n",
      "[3,  1710] loss: 0.721\n",
      "[3,  1810] loss: 0.848\n",
      "[3,  1910] loss: 0.836\n",
      "[3,  2010] loss: 0.768\n",
      "[3,  2110] loss: 0.788\n",
      "[3,  2210] loss: 0.866\n",
      "[3,  2310] loss: 0.558\n",
      "[3,  2410] loss: 0.780\n",
      "[3,  2510] loss: 0.808\n",
      "[3,  2610] loss: 0.784\n",
      "[3,  2710] loss: 0.853\n",
      "[3,  2810] loss: 0.769\n",
      "[3,  2910] loss: 0.767\n",
      "[3,  3010] loss: 0.804\n",
      "[3,  3110] loss: 0.666\n",
      "[3,  3210] loss: 0.853\n",
      "[3,  3310] loss: 0.740\n",
      "[3,  3410] loss: 0.548\n",
      "[3,  3510] loss: 0.715\n",
      "[3,  3610] loss: 0.775\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model, loss function, and optimizer\n",
    "model = CNNClassifier()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.0001)\n",
    "\n",
    "# Create data loader\n",
    "trainset = SampleDataset(X_train, Y_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size = 16, shuffle=True)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(3):  # Adjust number of epochs as needed\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 9:  # Print every 10 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 10))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "torch.save(model.state_dict(), 'models/model-L0-0.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNNClassifier(\n",
       "  (conv1): Conv1d(1, 16, kernel_size=(3,), stride=(1,))\n",
       "  (conv2): Conv1d(16, 16, kernel_size=(3,), stride=(1,))\n",
       "  (pool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): Linear(in_features=3040, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 439,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform inference with the saved model\n",
    "loaded_model = CNNClassifier()\n",
    "loaded_model.load_state_dict(torch.load('models/model-L0-0.pth'))\n",
    "loaded_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model output: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Example inference\n",
    "sample_input = torch.randn(1, 1, 768)  # Example input\n",
    "output = loaded_model(sample_input)\n",
    "print(\"Model output:\", output.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets do testing\n",
    "\n",
    "# Read pkl file \n",
    "with open('sst-val-768.pkl', 'rb') as f:\n",
    "    test_vectors = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt = []\n",
    "preds = []\n",
    "\n",
    "for i in range(len(test_vectors)):\n",
    "    input_vector = torch.tensor(test_vectors[i].reshape(1, 1, 768))\n",
    "    actual = val_dataset[i]['label']\n",
    "    output = loaded_model(input_vector) \n",
    "    if output < 0.5:\n",
    "        pred = 0\n",
    "    else:\n",
    "        pred = 1\n",
    "    gt.append(actual)\n",
    "    preds.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113\n"
     ]
    }
   ],
   "source": [
    "# Print the number of misclassifications\n",
    "misc = 0\n",
    "for i in range(len(gt)):\n",
    "    if gt[i] != preds[i]:\n",
    "        misc += 1\n",
    "\n",
    "print(misc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.84      0.86       428\n",
      "           1       0.85      0.90      0.88       444\n",
      "\n",
      "    accuracy                           0.87       872\n",
      "   macro avg       0.87      0.87      0.87       872\n",
      "weighted avg       0.87      0.87      0.87       872\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Generate classification report\n",
    "report = classification_report(gt, preds)\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
