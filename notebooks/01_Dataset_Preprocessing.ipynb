{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8e73124",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b582a5a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since stanfordnlp/imdb couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'plain_text' at /home/dhruv/.cache/huggingface/datasets/stanfordnlp___imdb/plain_text/0.0.0/e6281661ce1c48d982bc483cf8a173c1bbeb5d31 (last modified on Sat Apr 20 22:44:49 2024).\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"stanfordnlp/imdb\")\n",
    "train_dataset = dataset[\"train\"]\n",
    "#val_dataset = dataset[\"validation\"]\n",
    "test_dataset = dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c50e78a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for item in test_dataset:\n",
    "    if item['label'] == 0:\n",
    "        print(item['text'])\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e036e83",
   "metadata": {},
   "source": [
    "### Use BERT to have single vector for sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40e7b900",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dhruv/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:138: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 10010). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b5d049b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained BERT model and tokenizer\n",
    "# model_name = \"bert-base-uncased\"\n",
    "# tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "# model = BertModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41696e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_doc_embedding(document, tokenizer, model):\n",
    "#     # Tokenize input text\n",
    "#     inputs = tokenizer(document, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "#     # Forward pass through the model\n",
    "#     with torch.no_grad():\n",
    "#         outputs = model(**inputs)\n",
    "#     # Extract document embedding from hf model output\n",
    "#     document_embedding = outputs.last_hidden_state.mean(dim=1).squeeze(0)\n",
    "#     return document_embedding.reshape(-1, 1)\n",
    "\n",
    "def get_doc_embedding(document, model):\n",
    "    return model.encode(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "013a18db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768,)\n"
     ]
    }
   ],
   "source": [
    "# Example document from GLUE SST dataset\n",
    "document = \"those so-so films that could have been much better\"\n",
    "doc_emb = get_doc_embedding(document, model)\n",
    "print(doc_emb.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012258da",
   "metadata": {},
   "source": [
    "### Save the vectors in pkl files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "272616ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2049f8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_vectors = [] \n",
    "# Init dummylist to decode whihc split train/val/test\n",
    "dummy_list = test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d68b3254",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25000/25000 [1:48:33<00:00,  3.84it/s]  \n"
     ]
    }
   ],
   "source": [
    "for item in tqdm.tqdm(dummy_list):\n",
    "    sentence = item['text']\n",
    "    doc_emb = get_doc_embedding(sentence, model)\n",
    "    doc_vectors.append(doc_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "117bf0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the file and store the reduced sentence vectors\n",
    "with open('imdb-test-768.pkl', 'wb') as file:\n",
    "    # Serialize and write the variable to the file\n",
    "    pickle.dump(doc_vectors, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf15469",
   "metadata": {},
   "source": [
    "### Use PCA to reduce vector dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0de995d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc_vectors_np = np.array(doc_vectors)\n",
    "\n",
    "# # 768 is emb dimension\n",
    "# doc_vec_rehsaped = doc_vectors_np.reshape((len(dummy_list),768))\n",
    "\n",
    "# # Apply PCA for dimensionality reduction\n",
    "# pca = PCA(n_components = target_dimension)\n",
    "# pca.fit(doc_vec_rehsaped)\n",
    "\n",
    "# final_vecs = pca.transform(doc_vec_rehsaped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c60af5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
